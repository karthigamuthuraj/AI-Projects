{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to handle OpenMP duplicate library issue\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\AI-HandsOn\\AI-Projects\\aiproject\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess PDF\n",
    "pdf_path = 'jira_issues.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitting configuration\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=300)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store\n",
    "try:\n",
    "    faiss_db = FAISS.from_documents(documents, embeddings)\n",
    "    print(\"FAISS vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating FAISS vector store: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store\n",
    "try:\n",
    "    faiss_db = FAISS.from_documents(documents, embeddings)\n",
    "    print(\"FAISS vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating FAISS vector store: {e}\")\n",
    "\n",
    "# Function to generate a response from the language model\n",
    "def generate_response(prompt, max_length=500):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Set pad_token_id to eos_token_id if pad_token_id is not set\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Set pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_relevant_info(text, query):\n",
    "    # Extract relevant information based on the query\n",
    "    # This example assumes that queries contain specific keywords related to criteria\n",
    "    keywords = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    # Simple regex to find matching content in the text\n",
    "    pattern = '|'.join([f'\\\\b{keyword}\\\\b' for keyword in keywords])\n",
    "    matches = re.findall(pattern, text.lower())\n",
    "    return ' '.join(matches) if matches else \"No relevant information found.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and process query\n",
    "query = \"What are the acceptence criteria for the Sign n Language Detection?Explain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Results:\n",
      "page_content='Issue Key: KAN-1\n",
      "Summary: Acceptance Criteria for Sign Language Detection\n",
      "Description: Acceptance Criteria for Sign Language Detection * The system accurately detects and\n",
      "translates signs from images with at least 90% accuracy. * The user interface is intuitive and displays' metadata={'source': 'jira_issues.pdf', 'page': 0}\n",
      "======================================================\n",
      "Generated Response:======================================================\n",
      "are the the the what are the acceptence criteria for the sign n language detection explain criteria for sign language detection criteria for sign language detection the the the are for the what are the acceptence criteria for the sign language detection the the the for sign for sign for sign the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Perform similarity search with the query\n",
    "    faiss_results = faiss_db.similarity_search(query)\n",
    "    \n",
    "    if faiss_results:\n",
    "        # Get the first result and process it\n",
    "        first_result = faiss_results[0]\n",
    "        first_result_text = (first_result['page_content'] \n",
    "                             if isinstance(first_result, dict) and 'page_content' in first_result \n",
    "                             else str(first_result))\n",
    "        \n",
    "        print(\"FAISS Results:\")\n",
    "        print(first_result_text)\n",
    "        print(\"======================================================\")\n",
    "        # Generate a response using the language model\n",
    "        response_prompt = (\n",
    "            f\"You are the assitant \\n\\n\"\n",
    "             f\"From the following text, provide a single sentence answering the question:\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            f\"Text:\\n{first_result_text}\\n\\n\"\n",
    "            \"Provide a concise, one-sentence response directly addressing the question.\"\n",
    "        )\n",
    "        response = generate_response(response_prompt)\n",
    "        cleaned_response = extract_relevant_info(response, query)\n",
    "        \n",
    "        print(\"Generated Response:======================================================\")\n",
    "        print(response)\n",
    "    else:\n",
    "        print(\"FAISS similarity search returned no results.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during FAISS similarity search or response generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are the assitant \n",
      "\n",
      "From the following text, provide a single sentence answering the question:\n",
      "\n",
      "Question: What are the acceptence criteria for the Sign n Language Detection?Explain\n",
      "\n",
      "Text:\n",
      "page_content='Issue Key: KAN-1\n",
      "Summary: Acceptance Criteria for Sign Language Detection\n",
      "Description: Acceptance Criteria for Sign Language Detection * The system accurately detects and\n",
      "translates signs from images with at least 90% accuracy. * The user interface is intuitive and displays' metadata={'source': 'jira_issues.pdf', 'page': 0}\n",
      "\n",
      "Provide a concise, one-sentence response directly addressing the question.\n",
      "\n",
      "A:\n",
      "\n",
      "I think you are looking for the following:\n",
      "\n",
      "What are the acceptence criteria for the Sign Language Detection?\n",
      "\n",
      "The answer is:\n",
      "\n",
      "The system accurately detects and translates signs from images with at least 90% accuracy.\n",
      "\n",
      "The system is accurate, but it is not perfect. It is not perfect because it is not perfect for every sign. It is not perfect for every sign because it is not perfect for every sign.\n",
      "\n",
      "The user interface is intuitive and displays metadata={'source': 'jira_issues.pdf', 'page': 0}\n",
      "\n",
      "This is not a good answer. It is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not a good answer because it is not\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
